{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/bl4ckf0xk/ModelX_First_Order/blob/main/ModelX_Model.ipynb",
      "authorship_tag": "ABX9TyPzEOn5c7sjk/SWtC5uB2LS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bl4ckf0xk/ModelX_First_Order/blob/main/ModelX_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit, StratifiedKFold, train_test_split, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, recall_score, precision_score\n",
        "import joblib"
      ],
      "metadata": {
        "id": "bSddWgyukfDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "import shap"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzmndOz3kkxN",
        "outputId": "031f0601-eb52-4fbf-9009-13051fde609f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
            "Requirement already satisfied: numpy>=2 in /usr/local/lib/python3.12/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from shap) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/Dementia Prediction Dataset.csv'\n",
        "MODEL_OUT = '/content/dementia_nonmedical_model.pkl'\n",
        "RANDOM_STATE = 42\n",
        "TARGET = 'DEMENTED'\n",
        "SUBJECT_ID = 'NACCID'  # change if your identifier column has a different name"
      ],
      "metadata": {
        "id": "nPj5rIMSk8t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Data file not found at {path}. Upload it to Colab or set DATA_PATH correctly.\")\n",
        "    df = pd.read_csv(path)\n",
        "    return df"
      ],
      "metadata": {
        "id": "ugyYBlVslAI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NON_MEDICAL_WHITELIST = [\n",
        "    # Subject demographics\n",
        "    'BIRTHYR', 'BIRTHMO', 'SEX', 'HISPANIC', 'HISPOR', 'HISPORX',\n",
        "    'RACE', 'RACEX', 'RACESEC', 'RACESECX', 'RACETER', 'RACETERX',\n",
        "    'EDUC', 'MARISTAT', 'PRIMLANG', 'PRIMLANX', 'RESIDENC', 'HANDED', 'NACCLIVS', 'INDEPEND',\n",
        "    # Co-participant\n",
        "    'INBIRYR', 'INBIRMO', 'INSEX', 'INHISP', 'INHISPOR', 'INHISPOX', 'INRACE', 'INRACEX', 'INRASEC', 'INRASECX', 'INRATER', 'INRATERX', 'INEDUC', 'INRELTO', 'INRELTOX', 'NEWINF',\n",
        "    # Lifestyle\n",
        "    'TOBAC30', 'TOBAC100', 'SMOKYRS', 'PACKSPER', 'QUITSMOK', 'ALCOCCAS', 'ALCFREQ',\n",
        "    # Visit metadata\n",
        "    'NACCVNUM', 'NACCNVST', 'NACCAVST', 'NACCDAYS', 'NACCFDYS', 'PACKET', 'FORMVER', 'TELCOV', 'TELMOD',\n",
        "    # Family history (non-genetic fields)\n",
        "    'NACCFAM', 'NACCMOM', 'NACCDAD'\n",
        "]"
      ],
      "metadata": {
        "id": "w3ZoQMSvlD78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading data...')\n",
        "df = load_data(DATA_PATH)\n",
        "print('Rows:', len(df), 'Columns:', len(df.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5nvMH62lIPL",
        "outputId": "a6ebc708-5884-462d-93d7-b1e8769c205d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-175837067.py:4: DtypeWarning: Columns (20,22,24,26,28,41,44,46,48,51,61,63,65,67,69,71,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,134,156,165,176,179,189,217,220,222,224,226,228,230,232,234,236,238,240,242,244,246,248,250,252,254,256,258,260,262,264,266,268,270,272,382,397,399,401,419,421,423,432,445,454,494,574,605,613,638,674,690,704,707,710,715,727,738,744,746,803,804,809,810,811,812,820,831,833,835,837,843,904,959,960,961,969,970,971,972,982,1004,1007,1010) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 195196 Columns: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure target and subject id exist\n",
        "if TARGET not in df.columns:\n",
        "    raise KeyError(f\"Target column '{TARGET}' not found in dataset.\")\n",
        "if SUBJECT_ID not in df.columns:\n",
        "    raise KeyError(f\"Subject ID column '{SUBJECT_ID}' not found in dataset.\")\n",
        "\n",
        "# Reduce to columns we care about (keep subject id and target)\n",
        "available_features = [c for c in NON_MEDICAL_WHITELIST if c in df.columns]\n",
        "print(f'Using {len(available_features)} non-medical features (of {len(NON_MEDICAL_WHITELIST)} whitelist)')\n",
        "\n",
        "# Warn about missing whitelist columns\n",
        "missing = set(NON_MEDICAL_WHITELIST) - set(available_features)\n",
        "if missing:\n",
        "    print('Warning: The following whitelist columns were not found in your CSV and will be skipped:')\n",
        "    print(sorted(list(missing)))\n",
        "\n",
        "keep_cols = [SUBJECT_ID, TARGET] + available_features\n",
        "df = df[keep_cols].copy()"
      ],
      "metadata": {
        "id": "yEQHJk-AloVb",
        "outputId": "cc6810eb-e324-40bb-f333-4d4f0253298d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 55 non-medical features (of 55 whitelist)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle NACC special codes -> convert common missing codes to NaN"
      ],
      "metadata": {
        "id": "qSKY5goUlzh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Typical NACC codes: -4 = Not applicable, -1 or 9 = Unknown, 88/99 = other/missing depending on variable\n",
        "NA_CODES = [-4, -1, 8, 9, 88, 95, 96, 97, 98, 99, 999]\n",
        "for v in df.columns:\n",
        "    if df[v].dtype.kind in 'biufc':\n",
        "        df[v] = df[v].replace(NA_CODES, np.nan)\n",
        "    else:\n",
        "        # for object/string fields, keep as-is and handle missing later\n",
        "        df[v] = df[v].replace([str(x) for x in NA_CODES], np.nan)\n",
        "\n",
        "# Also treat target code '9' explicitly as NaN (unknown)\n",
        "df[TARGET] = df[TARGET].replace(9, np.nan)\n",
        "\n",
        "# Drop rows where target is missing\n",
        "df = df[df[TARGET].notna()].copy()\n",
        "print('After dropping unknown target, rows:', len(df))"
      ],
      "metadata": {
        "id": "jSqXpbV_l2fG",
        "outputId": "261f6fc2-4042-4d8f-dc63-3b03ca4b8646",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After dropping unknown target, rows: 195196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering"
      ],
      "metadata": {
        "id": "Cx3y9BiJl8U2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AGE if present; compute age-at-visit if possible using BIRTHYR and an approximate VISITYR if available.\n",
        "# If NACCAGE exists, prefer it. Otherwise derive AGE from BIRTHYR with caveat.\n",
        "if 'NACCAGE' in df.columns:\n",
        "    df['AGE'] = df['NACCAGE']\n",
        "elif 'BIRTHYR' in df.columns and 'NACCVNUM' in df.columns:\n",
        "    # cheaper approximation: assume visit year unknown; fallback to age buckets via EDUC if needed\n",
        "    df['AGE'] = np.nan\n",
        "else:\n",
        "    df['AGE'] = np.nan\n",
        "\n",
        "# Pack-years and simple booleans\n",
        "if 'PACKSPER' in df.columns and 'SMOKYRS' in df.columns:\n",
        "    df['PACK_YEARS'] = df['PACKSPER'] * df['SMOKYRS']\n",
        "else:\n",
        "    df['PACK_YEARS'] = np.nan\n",
        "\n",
        "if 'TOBAC30' in df.columns and 'TOBAC100' in df.columns:\n",
        "    df['EVER_SMOKE'] = ((df['TOBAC30'] == 1) | (df['TOBAC100'] == 1)).astype('Int64')\n",
        "else:\n",
        "    df['EVER_SMOKE'] = pd.Series([pd.NA] * len(df))\n",
        "\n",
        "# Heavy alcohol heuristic\n",
        "if 'ALCFREQ' in df.columns:\n",
        "    # interpret codes: (user should adjust according to their codebook)\n",
        "    # treat high frequency codes (e.g., weekly/daily) as heavy\n",
        "    df['HEAVY_ALCOHOL'] = df['ALCFREQ'].apply(lambda x: 1 if (pd.notna(x) and float(x) >= 5) else 0).astype('Int64')\n",
        "else:\n",
        "    df['HEAVY_ALCOHOL'] = pd.Series([pd.NA] * len(df))\n",
        "\n",
        "# Years in study\n",
        "if 'NACCDAYS' in df.columns:\n",
        "    df['YEARS_IN_STUDY'] = df['NACCDAYS'] / 365.25\n",
        "else:\n",
        "    df['YEARS_IN_STUDY'] = np.nan\n",
        "\n",
        "# Lives alone bool from NACCLIVS (if 1 = lives alone in your version)\n",
        "if 'NACCLIVS' in df.columns:\n",
        "    df['LIVES_ALONE'] = df['NACCLIVS'].apply(lambda x: 1 if x == 1 else 0).astype('Int64')\n",
        "else:\n",
        "    df['LIVES_ALONE'] = pd.Series([pd.NA] * len(df))\n",
        "\n",
        "# Final feature list (keep engineered features)\n",
        "engineered = ['AGE', 'PACK_YEARS', 'EVER_SMOKE', 'HEAVY_ALCOHOL', 'YEARS_IN_STUDY', 'LIVES_ALONE']\n",
        "for e in engineered:\n",
        "    if e not in df.columns:\n",
        "        df[e] = np.nan"
      ],
      "metadata": {
        "id": "Xo0srCM2mBXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final feature set building (numeric vs categorical)"
      ],
      "metadata": {
        "id": "lje2VjF5mGMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start from available_features (from whitelist) and add engineered numeric features\n",
        "features = available_features.copy()\n",
        "# Remove potential duplicates if present\n",
        "for c in engineered:\n",
        "    if c not in features:\n",
        "        features.append(c)\n",
        "\n",
        "# Remove subject id and target if present mistakenly\n",
        "features = [f for f in features if f not in [SUBJECT_ID, TARGET]]\n",
        "\n",
        "# Separate numeric and categorical heuristically\n",
        "numeric_feats = []\n",
        "categorical_feats = []\n",
        "for f in features:\n",
        "    if pd.api.types.is_numeric_dtype(df[f]) or f in engineered:\n",
        "        numeric_feats.append(f)\n",
        "    else:\n",
        "        categorical_feats.append(f)\n",
        "\n",
        "print('Numeric features:', len(numeric_feats))\n",
        "print('Categorical features:', len(categorical_feats))"
      ],
      "metadata": {
        "id": "7kvtnCG-mJi6",
        "outputId": "3ce829f5-a34a-45f6-de23-f3085f750b73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric features: 50\n",
            "Categorical features: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing pipeline"
      ],
      "metadata": {
        "id": "Xr2yBHLWmNkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_feats),\n",
        "        ('cat', categorical_transformer, categorical_feats)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")"
      ],
      "metadata": {
        "id": "xcgyi6UAmQUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[features].copy()\n",
        "y = df[TARGET].astype(int)\n",
        "groups = df[SUBJECT_ID]\n",
        "\n",
        "# Group-aware train-test split: ensure subjects do not leak across splits\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
        "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
        "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "print('Train subjects:', groups.iloc[train_idx].nunique(), 'Test subjects:', groups.iloc[test_idx].nunique())"
      ],
      "metadata": {
        "id": "KD1n_-QgmlfA",
        "outputId": "027c7a9b-2257-4f73-f073-4d836badfadc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train subjects: 42029 Test subjects: 10508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model pipeline (XGBoost) + hyperparameter search"
      ],
      "metadata": {
        "id": "9L0Ky3EMmp7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_clf = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='auc',\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('clf', xgb_clf)])\n",
        "\n",
        "# Hyperparameter space for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'clf__n_estimators': [100, 250, 500],\n",
        "    'clf__max_depth': [3, 5, 7],\n",
        "    'clf__learning_rate': [0.01, 0.03, 0.1],\n",
        "    'clf__subsample': [0.6, 0.8, 1.0],\n",
        "    'clf__colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'clf__gamma': [0, 1, 5]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "rsearch = RandomizedSearchCV(\n",
        "    pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring='roc_auc',\n",
        "    cv=cv,\n",
        "    verbose=2,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print('Starting hyperparameter search...')\n",
        "rsearch.fit(X_train, y_train)\n",
        "print('Best params:', rsearch.best_params_)"
      ],
      "metadata": {
        "id": "BRZKlSqXmtpq",
        "outputId": "0151ed73-442b-4305-b61f-a1f9a5758d25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting hyperparameter search...\n",
            "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = rsearch.best_estimator_\n",
        "\n",
        "# Predict probabilities and classes\n",
        "y_test_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "auc = roc_auc_score(y_test, y_test_prob)\n",
        "rec = recall_score(y_test, y_test_pred)\n",
        "prec = precision_score(y_test, y_test_pred)\n",
        "\n",
        "print(f'Test AUC: {auc:.4f}')\n",
        "print(f'Test Recall (sensitivity): {rec:.4f}')\n",
        "print(f'Test Precision: {prec:.4f}')\n",
        "print('\\nClassification report:\\n')\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "print('\\nConfusion matrix:\\n')\n",
        "print(confusion_matrix(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "4YnLDeaRm0bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature importance (approximate) - extract feature names after preprocessing"
      ],
      "metadata": {
        "id": "GPfxnlkgm4Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preproc = best_model.named_steps['preprocessor']\n",
        "num_names = numeric_feats\n",
        "cat_names = []\n",
        "if len(categorical_feats) > 0:\n",
        "    ohe = preproc.named_transformers_['cat'].named_steps['onehot']\n",
        "    # scikit-learn 1.0+ has get_feature_names_out\n",
        "    try:\n",
        "        cat_names = list(ohe.get_feature_names_out(categorical_feats))\n",
        "    except Exception:\n",
        "        # fallback for older versions\n",
        "        cat_names = [f for f in categorical_feats]\n",
        "\n",
        "feature_names = num_names + cat_names\n",
        "\n",
        "try:\n",
        "    importances = best_model.named_steps['clf'].feature_importances_\n",
        "    fi = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "    print('\\nTop 20 feature importances:')\n",
        "    print(fi.head(20))\n",
        "except Exception as e:\n",
        "    print('Could not extract feature importances:', e)"
      ],
      "metadata": {
        "id": "BYeTTCn5m6-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(best_model, MODEL_OUT)\n",
        "print('Saved trained pipeline to', MODEL_OUT)"
      ],
      "metadata": {
        "id": "bGjg4HsBm-Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_subject(model_path, row_dict):\n",
        "    \"\"\"row_dict: dict mapping feature names to values (the same features used in training)\n",
        "    Returns: dict with probability and class label (threshold 0.5)\n",
        "    \"\"\"\n",
        "    model = joblib.load(model_path)\n",
        "    X_in = pd.DataFrame([row_dict])\n",
        "    prob = model.predict_proba(X_in)[:, 1][0]\n",
        "    pred = int(prob > 0.5)\n",
        "    return {'probability': float(prob), 'prediction': pred}\n",
        "\n",
        "# Example usage (uncomment and replace example values):\n",
        "# example = {f: None for f in features}\n",
        "# example['SEX'] = 1\n",
        "# example['EDUC'] = 12\n",
        "# print(predict_subject(MODEL_OUT, example))"
      ],
      "metadata": {
        "id": "oPCYsCrnnAQx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}